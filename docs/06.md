

# 6 概率

概率论是处理机会和不确定性的数学分支。它是统计学基础的重要组成部分，因为它为我们提供了描述不确定事件的数学工具。概率研究的兴起部分是由于对理解像纸牌或骰子这样的机会游戏的兴趣。这些游戏提供了许多统计概念的有用例子，因为当我们重复这些游戏时，不同结果的可能性仍然(大部分)相同。然而，有一些关于概率意义的深层问题，我们在这里不讨论；如果你有兴趣了解这个有趣的话题及其历史，请看最后的推荐读物。



## 6.1 什么是概率？

非正式地，我们通常认为概率是一个描述某个事件发生的可能性的数字，范围从零(不可能)到一(确定)。有时概率会用百分比来表示，范围从0到100，例如天气预报预测今天有20%的机会下雨。在每种情况下，这些数字都表达了特定事件发生的可能性，从绝对不可能到绝对肯定。

为了形式化概率论，我们首先需要定义几个术语:

*   一个**实验**是产生或观察一个结果的任何活动。例如抛硬币、掷骰子或尝试新的工作路线，看看是否比旧路线快。
*   **样本空间**是实验可能结果的集合。我们用一组弯弯曲曲的括号来表示它们。对于抛硬币，样本空间是{正面，反面}。对于六面骰子，样本空间是可能出现的每个数字:{1，2，3，4，5，6}。对于到达工作地点所需的时间，样本空间是所有可能的大于零的实数(因为到达某个地方不会花费负的时间，至少目前不会)。我们不会费心去写出括号内的所有数字。
*   一个**事件**是样本空间的子集。原则上，它可能是样本空间中的一个或多个可能的结果，但这里我们将主要关注由一个可能的结果组成的*基本事件*。例如，这可能是在一次抛硬币中获得正面，在掷骰子中掷出4，或者花21分钟通过新路线回家。

既然我们有了这些定义，我们就可以勾勒出概率的形式特征，它是由俄罗斯数学家安德烈·科尔莫戈罗夫首先定义的。如果一个值*成为一个概率，那么这些就是这个值*必须具备的特征。假设我们有一个由N个独立事件定义的样本空间，<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><msub><mi>E</mi><mn>1</mn></msub><mo>，</mo><msub><mi>E</mi><mn>2</mn></msub><mo>，</mo> <mi>。</mi> <mi>。</mi> <mi>。</mi> <mo>，</mo><msub><mi>E</mi><mi>N</mi></msub></mrow><annotation encoding="application/x-tex">{ E _ 1，E_2，...，E_N}</annotation></semantics></math> ，<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>是随机变量，表示发生了哪个事件。<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><msub><mi>E</mi><mi>I</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(X = E _ I)</annotation></semantics></math>是事件发生的概率

*   概率不能为负:<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><mo>=</mo><msub><mi>E</mi><mi>I</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">P(X = E _ I)\ ge 0</annotation></semantics></math>
*   样本空间中所有结果的总概率为1；也就是说，如果，如果我们把每个Ei的概率加起来，它们的总和一定是1。我们可以用求和符号<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mo>∑</mo><annotation encoding="application/x-tex">\ sum</annotation></semantics></math>:<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><munderover><mo>∑</mo><mrow><mi>I</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi></mrow></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X<mo><msub><mi>E</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mo></mi></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo> <mi>。</mi> <mi>。</mi><mo>+</mo><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>=</mo><msub><mi>e</mi><mi>n</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum_{i=1}^n{p(x=e_i)} = p(x = e...+ P(X=E_N) = 1</annotation> 这些必须加在一起。”</semantics></math>

*   任何单个事件的概率都不能大于一:<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>X</mi><msub><mi>E</mi><mi>I</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">P(X = E _ I)\ le 1</annotation></semantics></math>。这是前一点所暗示的；因为它们的总和必须是1，而且它们不能是负的，那么任何特定的概率都不能超过1。





## 我们如何确定概率？

现在我们知道了概率是什么，我们如何实际计算出任何特定事件的概率是多少呢？



### 6.2.1 个人信念

假设我问你，如果伯尼·桑德斯是民主党提名人而不是希拉里，他赢得2016年总统大选的概率是多少？我们实际上不能做实验来寻找结果。然而，大多数了解美国政治的人会愿意至少对这一事件的可能性提供一个猜测。在许多情况下，个人知识和/或意见是我们确定事件概率的唯一指导，但这在科学上并不令人满意。





### 6.2.2 经验频率

另一种确定事件发生概率的方法是多次做实验，并计算每个事件发生的频率。根据不同结果的相对频率，我们可以计算出每个结果的概率。例如，我们想知道旧金山下雨的概率。我们首先必须定义这个实验——假设我们将查看2017年每天的国家气象局数据，并确定旧金山市中心气象站是否下雨。根据这些数据，2017年有73个雨天。为了计算旧金山下雨的概率，我们简单地用下雨的天数除以统计的天数(365)，得出2017年旧金山的雨)= 0.2。

我们怎么知道经验概率给了我们正确的数字？这个问题的答案来自于*大数定律*，表明经验概率会随着样本量的增加而向真概率逼近。我们可以通过模拟大量的硬币投掷，并查看我们对每次投掷后正面概率的估计来了解这一点。我们将在后面的章节中花更多的时间讨论模拟；现在，假设我们有一种计算方法来为每次抛硬币产生随机结果。

图 [6.1](#fig:ElectionResults) 的左图显示，随着样本数量(即抛硬币试验)的增加，估计的正面概率收敛到真实值0.5。但是，请注意，当样本量很小时，估计值可能与真实值相差很远。这方面的一个真实例子是2017年阿拉巴马州美国参议院特别选举，共和党人罗伊·摩尔与民主党人道格·琼斯进行了对决。图 [6.1](#fig:ElectionResults) 的右侧面板显示了随着计票数量的增加，当晚每位候选人的相对票数。晚上早些时候的计票尤其不稳定，从琼斯最初的大幅领先到摩尔长期领先，直到最后琼斯领先赢得比赛。

![Left: A demonstration of the law of large numbers.  A coin was flipped 30,000 times, and after each flip the probability of heads was computed based on the number of heads and tail collected up to that point.  It takes about 15,000 flips for the probability to settle at the true probability of 0.5\. Right: Relative proportion of the vote in the Dec 12, 2017 special election for the US Senate seat in Alabama, as a function of the percentage of precincts reporting. These data were transcribed from https://www.ajc.com/news/national/alabama-senate-race-live-updates-roy-moore-doug-jones/KPRfkdaweoiXICW3FHjXqI/](../media/file38.png)

图6.1:左图:大数定律的演示。一枚硬币被抛了30，000次，每次抛完之后，正面的概率是根据当时收集到的正面和反面的数量来计算的。大约需要15，000次翻转，概率才会稳定在0.5的真实概率。右图:2017年12月12日阿拉巴马州美国参议院席位特别选举中投票的相对比例，作为选区报告百分比的函数。这些数据转录自[https://www . AJC . com/news/national/Alabama-Senate-race-live-updates-Roy-Moore-Doug-Jones/kprfkdaweixizw3fhjxqi/](https://www.ajc.com/news/national/alabama-senate-race-live-updates-roy-moore-doug-jones/KPRfkdaweoiXICW3FHjXqI/)

这两个例子表明，虽然大样本最终会收敛于真实概率，但小样本的结果可能相差甚远。不幸的是，许多人忘记了这一点，过度解读小样本的结果。心理学家丹尼·卡尼曼和阿莫斯·特沃斯基称之为“小数字定律”( T0 )( T1 ),他们指出，人们(甚至是训练有素的研究人员)经常表现得好像大数定律甚至适用于小样本，过于相信基于小数据集的结果。我们将在整个课程中看到一些例子，说明基于小样本生成的统计结果是多么不稳定。





### 6.2.3 经典概率

我们中的任何人都不太可能将硬币抛过数万次，但我们仍然愿意相信抛头的概率是0.5。这反映了使用另一种方法来计算概率，我们称之为经典概率。在这种方法中，我们直接根据对情况的了解来计算概率。

经典概率起源于对骰子和纸牌等概率游戏的研究。一个著名的例子来自一个名叫谢瓦利埃·德·梅雷的法国赌徒遇到的一个问题。de Méré玩了两种不同的骰子游戏:在第一种游戏中，他赌的是在四个六面骰子上至少有一个六的机会，而在第二种游戏中，他赌的是在24个双骰子上至少有一个双六的机会。他期望在这两次赌博中都赢到钱，但是他发现虽然平均来说他在第一次赌博中赢了钱，但是当他多次进行第二次赌博时，他实际上平均来说输了钱。为了理解这一点，他求助于他的朋友，数学家布莱士·帕斯卡，他现在被认为是概率论的创始人之一。

用概率论怎么理解这个问题？在经典概率中，我们从假设样本空间中的所有基本事件都是同等可能的开始；也就是说，当你掷骰子时，每种可能的结果({1，2，3，4，5，6})发生的可能性是相等的。(不允许装骰子！)考虑到这一点，我们可以将任何单个结果的概率计算为除以可能结果的数量:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>o</mi><mi>u</mi><mi>t<mi>c</mi><mi>o</mi><mi>m</mi><msub><mi>e</mi><mi>I</mi></msub><mo stretchy="true" form="postfix">)</mo></mi></mrow><mo>=</mo><mfrac><mn>1</mn></mfrac></mrow></semantics></math>

对于六面骰子，每个单独结果的概率是1/6。

这很好，但德梅雷对更复杂的事件感兴趣，比如多次掷骰子会发生什么。我们如何计算复杂事件(即单个事件的*联合*)的概率，比如在第一次*中掷出6或者第二次*中掷出6？我们用<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mo>∨</mo><annotation encoding="application/x-tex">\杯</annotation></semantics></math> 符号在数学上表示事件的联合:例如， 如果在第一次投掷上掷出一个六的概率称为<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>R</mi><mi>o</mi><mi>l</mi><mi>l</mi><msub><mn>6</mn><mrow><mi>t</mi><mi>h</mi><mi>R</mi><mi>o</mi><mi>w</mi></mrow></msub></mrow></mrow></semantics></math> 第二次掷出一个六的概率是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>R</mi><mi>o</mi><mi>l</mi><mi>l</mi><msub><mn>6</mn><mrow><mi>h 那么这个工会就简称为<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>R</mi><mi>o</mi><mi>l</mi><msub><mn>6</mn><mrow><mi>t</mi><mi>h</mi><mi>R</mi> <mi>r</mi><mi>o</mi><mi>w</mi><mn>2</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(roll 6 _ { throw 1 } \ cup roll 6 _ { throw 2 })</annotation></semantics></math>。</mi></mrow></msub></mrow></mrow></semantics></math>

de Méré认为(不正确，我们将在下面看到),他可以简单地将单个事件的概率相加来计算组合事件的概率，这意味着在第一次或第二次掷骰子时掷出6的概率计算如下:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">【r】 <mi><mi>= 1/6</mi></mi></mo></mrow></mrow></semantics><math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>或【o】</mi></mrow></mrow></semantics></math>

<semantics><mrow><mi>【d】</mi><mi>【e】</mi><mi>【m】</mi><mi>【是<mi>【r】</mi><mi>【是】 错误:<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi><mi><mi>或</mi> o</mi><mi>【w】</mi><mn>【2】</mn></mi></mrow><mo stretchy="true" form="postfix">)</mo></semantics></math></mi></mi></mrow><mo>=</mo> <mo>+</mo><mi><mrow><mo stretchy="true" form="prefix"><mi>【r</mi><mi><mi><mn>6</mn><mo>=【T1191】<mi>/</mi></mo></mi></mi></mo></mrow><annotation encoding="application/x-tex">p(卷6 _ {扔1 } \ cup</annotation></mi></semantics>

de Méré基于这个错误的假设进行推理，即四次掷骰中至少有一次六的概率是每次单独掷出的概率之和:<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mn>4</mn><mo>*</mo><mfrac><mn>1</mn><mn>6</mn></mfrac><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">4 * \ frac { 1 } { 6 } = \ frac { 2同样，他推断，既然掷出两个骰子时出现双六的概率是1/36，那么在24次掷出的两个骰子中至少有一个双六的概率将是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mn>24</mn><mo>*</mo><mfrac>T34】1<mn>36</mn></mfrac><mo>=</mo><mfrac><mn>2</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">然而，虽然他在第一次下注时一直赢钱，但在第二次下注时却输了。怎么回事？</annotation></semantics></math></annotation></semantics></math>

为了理解de Méré的错误，我们需要介绍一些概率论的规则。第一个是减法的*法则，它说某个事件A *而不是*发生的概率是1减去该事件发生的概率:*

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【a】</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></semantics>

其中<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">\ neg A</annotation></semantics></math>表示“不是A”。这条规则直接来源于我们上面讨论的公理；因为A和<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">\ neg A</annotation></semantics></math>是唯一可能的结果，那么它们的总概率之和必须为1。例如，如果在单次投掷中掷出一个1的概率是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mfrac><mn>1</mn></mfrac><annotation encoding="application/x-tex">\ frac { 1 } { 6 }</annotation></semantics></math>，那么掷出一个以外的任何东西的概率是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mfrac><mn>5</mn><mn>6</mn></mfrac><annotation encoding="application/x-tex">\ frac { 5 } { 6 }</annotation></semantics></math>

第二个规则告诉我们如何计算联合事件的概率——即两个事件同时发生的概率。我们将此称为*交点*，用<math display="inline"><semantics><annotation encoding="application/x-tex">\ cap</annotation></semantics></math>符号表示；由此，<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A \ cap B)</annotation></semantics></math>表示A和B都发生的概率。我们将集中讨论这个规则的一个版本，它告诉我们在两个事件相互独立的特殊情况下如何计算这个量；我们稍后将确切了解*独立性*的概念是什么意思，但是现在我们可以理所当然地认为两次掷骰子是独立的事件。我们通过简单地将单个事件的概率相乘来计算两个独立事件相交的概率:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi>∩<mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>*</mo><mi>P</mi> <mrow><mo stretchy="true" form="prefix">\ text {当且仅当A和B是独立的}</mo></mrow></mrow></semantics></math> 因此，两次掷出6的概率是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mfrac><mn>1</mn><mn>6</mn></mfrac><mo>*</mo><mfrac><mn>1</mn><mn>6</mn></mfrac><mo>=</mo><mfrac><mn>1</mn></mfrac></mrow></semantics></math>

第三条规则告诉我们如何将概率加在一起——正是在这里，我们看到了德·梅勒错误的根源。加法法则告诉我们，要获得两个事件中任何一个发生的概率，我们将各个概率相加，然后减去两个事件同时发生的可能性:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi>∩<mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi></mrow><mo>+</mo><mi>P</mi><annotation encoding="application/x-tex">P(A \ cup B)= P(A)+P(B)-P(A \ cap B)</annotation></mrow></semantics></math>从某种意义上说，这阻止了我们对那些实例进行两次计数，这也是该规则与de Méré错误计算的区别所在。 假设我们想找出两次投掷中任何一次掷出6的概率。根据我们的规定:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">【r】<mi><mn>【2】</mn></mi></mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi><mrow><mo stretchy="true" form="prefix">(</mo></mrow> <mo>+</mo><mi>【p104】<mrow><mo stretchy="true" form="prefix"><mi>【r】<mi>【T1112】【l】</mi></mi></mo></mrow> 【T1137】-【T1138】<mi><mrow>【T1142】<mi><mi><mi><mi>【l148】 <mi>w</mi><mn>【2】</mn></mi></mi></mi></mi></mrow><mo stretchy="true" form="postfix">)</mo><annotation encoding="application/x-tex">p(卷6 _ {扔1} \ cup 6 _ {扔2}) = P(卷6 _ {扔1 }))</annotation></mi></mi></mi></semantics>

![Each cell in this matrix represents one outcome of two throws of a die, with the columns representing the first throw and the rows representing the second throw. Cells shown in red represent the cells with a six in either the first or second throw; the rest are shown in blue.](../media/file39.png)

图6.2:这个矩阵中的每个单元格代表一个骰子两次投掷的结果，列代表第一次投掷，行代表第二次投掷。显示为红色的单元格表示第一次或第二次投掷中有6的单元格；其余的用蓝色显示。

让我们使用图形描述来获得这个规则的不同视图。图 [6.2](#fig:ThrowMatrix) 显示了代表两次投掷结果的所有可能组合的矩阵，并突出显示了第一次或第二次投掷中包含6的单元格。如果你数一下红色的细胞，你会看到有11个这样的细胞。这说明了为什么加法法则给出了与德·梅雷不同的答案；如果我们像他那样简单地把两次投掷的概率加在一起，那么我们会两次都数(6，6)，而实际上只应该数一次。





### 6.2.4 解决德梅雷的问题

布莱士·帕斯卡利用概率法则想出了一个解决德·梅雷问题的办法。首先，他意识到计算一个组合中至少一个事件发生的概率是复杂的，而计算几个事件中某件事不发生的概率相对容易——它只是单个事件概率的乘积。因此，他不是计算四次投掷中至少有一次六的概率，而是计算所有投掷中没有六的概率:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">【不要在四辆劳斯莱斯里塞思</mtext><mo>=】</mo> <mfrac><mn>【5】</mn><mn>【6】</mn></mfrac>=<mo minsize="2.4" maxsize="2.4" stretchy="false" form="prefix"><mfrac></mfrac></mo></mrow></mrow></semantics></math>

然后，他利用四次掷骰中没有六点的概率是四次掷骰中至少有一个六点的补数这一事实(因此它们的总和必须是一)，并利用减法规则来计算感兴趣的概率:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">四卷中至少有一卷六卷</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>1</mn><mo>-</mo><mo minsize="2.4" maxsize="2.4" stretchy="false" form="prefix">(</mo><mfrac><mn>5</mn><mn>6</mn></mfrac><msup><mo minsize="2.4" maxsize="2.4" stretchy="false" form="postfix">)</mo><mn>4</mn></msup><mo>=】</mo></mrow></semantics></math>

de Méré赌他会在四次掷骰子中至少掷出一个6，这个概率大于0.5，这解释了为什么de Méré平均在这个赌注上赚了钱。

但是德梅雷的第二次赌注呢？帕斯卡使用了同样的伎俩:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">24卷中无双六</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo minsize="2.4" maxsize="2.4" stretchy="false" form="prefix">(</mo><mfrac><mn>35</mn><mn>36</mn></mfrac><msup>【T24)<mn>24</mn></msup><mo>=</mo><mn>0.509 <mn>= \bigg(\frac{35}{36}\bigg)^{24}=0.509</mn></mn></mrow></semantics></math><math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix"><mtext mathvariant="normal">24卷</mtext></mo></mrow><mo>=</mo><mn>1</mn>-<mo minsize="2.4" maxsize="2.4" stretchy="false" form="prefix">(</mo><mfrac><mn>35</mn><mn>36</mn></mfrac></mrow></semantics></math>

这一结果的概率略低于0.5，这表明了为什么德梅雷在这场赌博中平均输钱。







## 6.3 概率分布

*概率分布*描述了实验中所有可能结果的概率。例如，2018年1月20日，篮球运动员斯蒂芬·库里在对阵休斯顿火箭队的比赛中，4次罚球中只有2次。我们知道库里整个赛季罚球的总概率是0.91，所以他在一场比赛中只有50%的罚球命中率似乎不太可能，但这到底有多不可能？我们可以使用理论概率分布来确定这一点；在本书中，我们会遇到许多这样的概率分布，每一种都适合于描述不同类型的数据。在这种情况下，我们使用*二项式*分布，它提供了一种方法来计算一些成功的概率，这些成功或失败的试验(称为“伯努利试验”)中，给定每个试验的一些已知的成功概率。这种分布被定义为:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>【p】</mi><mi>【K9】<mo>；</mo><mi><mo>，</mo><mi><mo stretchy="true" form="postfix">)</mo></mi><mo>=</mo><mi><mrow>n，p)= p(x = k)= \ binom { n } { k } p ^ k(1-p)^ { n-k }</mrow>

这是指当成功概率为p时，n次试验中k次成功的概率，你可能不熟悉<math display="inline"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>n</mi><mi>k</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">\ binom { n } { k }</annotation></semantics></math>，简称为*二项式系数*。二项式系数也被称为“n-choose-k ”,因为它描述了从n个项目中选择k个项目的不同方法的数量。二项式系数计算如下:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>n</mi><mi>k</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>n</mi><mi>！</mi></mrow> <mrow><mi>k</mi> <mi>！</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>—</mo>—T36】k<mo stretchy="true" form="postfix">)</mo></mrow><mi>！</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">【binom { n } { k } = \ frac { n！}{k！(n-k)！}</annotation></semantics></math> 感叹号(！)指的是*数的阶乘*:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>！</mi><mo>=</mo><munderover><mrow><mo>=</mo></mrow> <mi>。</mi> <mi>。</mi><mo>*</mo><mn><semantics></semantics></mn></munderover></mrow><annotation encoding="application/x-tex">n！= \prod_{i=1}^n i = n*(n-1)*-我...。*2*1</annotation></semantics></math>

乘积运算符<math display="inline"><semantics><mo>∏</mo><annotation encoding="application/x-tex">\ prod</annotation></semantics></math>与求和运算符<math display="inline"><semantics><annotation encoding="application/x-tex">\ sum</annotation></semantics></math>类似，只是前者是乘法而不是加法。在这种情况下，它是将从1到<math display="inline"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>的所有数字相乘。

以斯蒂芬·库里的罚球为例:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>【p】</mi><mn><mrow><mo>；</mo><mn><mo>、</mo><mn>【0.91】</mn><mo><mo>=</mo><mo stretchy="true" form="prefix">(【t】4 . 0 . 91)= \ binom { 4 } { 2 } 0.91 ^ 2(1-0.91)^ { 4-2 } = 0.040</mo></mo></mn></mrow></mn></mrow></semantics></math>

这表明，鉴于库里的整体罚球命中率，他不太可能在4次罚球中只命中2次。这正说明了不可能的事情确实在现实世界中发生了。



### 6.3.1 累积概率分布

通常我们不仅想知道一个特定值的可能性有多大，还想知道找到一个与特定值一样极端或比特定值更极端的值的可能性有多大；当我们在第9章讨论假设检验时，这将变得非常重要。要回答这个问题，我们可以用一个*累积*的概率分布；标准概率分布告诉我们某个特定值的概率，而累积分布告诉我们某个值大于或等于(或小于或等于)某个特定值的概率。

在罚球的例子中，我们可能想知道:假设斯蒂芬库里的总罚球概率为0.91，那么他在四次罚球中命中2次*或更少*的概率是多少。为了确定这一点，我们可以简单地使用二项式概率方程，插入k的所有可能值，并将它们加在一起:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【k】</mi><mn><mo>+</mo><mi><mrow><mo stretchy="true" form="prefix"><mi>【k57】<mo>=</mo><mn>0</mn></mi></mo></mrow></mi></mn></mrow></mrow></semantics>

在许多情况下，可能结果的数量太大，我们无法通过列举所有可能的值来计算累积概率；幸运的是，它可以直接计算任何理论概率分布。表 [6.1](#tab:freethrow) 显示了上例中每种可能的罚球成功次数的累积概率，从中我们可以看出，库里4次罚球中2次或更少罚球落地的概率为0.043。

<caption>Table 6.1: Simple and cumulative probability distributions for number of successful free throws by Steph Curry in 4 attempts.</caption>
| 数字成功 | 可能性 | 累积概率 |
| --- | --- | --- |
| Zero | Zero | Zero |
| one | Zero point zero zero three | Zero point zero zero three |
| Two | Zero point zero four | Zero point zero four three |
| three | Zero point two seven one | Zero point three one four |
| four | Zero point six eight six | One |





## 6.4 条件概率

到目前为止，我们把自己限制在简单的概率上——也就是说，单一事件或事件组合的概率。然而，我们经常希望在已知其他事件已经发生的情况下确定某个事件的概率，这被称为*条件概率*。

我们以2016年美国总统大选为例。我们可以用两个简单的概率来描述选民。首先，我们知道美国一个选民隶属于共和党的概率:<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>R</mi><mi>e</mi><mi>p</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>I</mi><mi>c</mi><mi>a</mi><mi>n</mi>我们也知道选民投票支持川普的概率:<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>p</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>T</mi><mi>r</mi><mi>u</mi><mi>m</mi><mi>p</mi><mi>v</mi>【T60<mi>T</mi><mi>e</mi><mi>然而，假设我们想知道以下问题:假设一个人是共和党人，他投票给唐纳德·特朗普*的概率是多少？*</mi></mrow></mrow></semantics></math></mrow></mrow></semantics></math>

要计算给定B的条件概率(我们写为<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A | B)</annotation></semantics></math>，“A的概率，给定B”)，我们需要知道*联合概率*(即A和B都发生的概率

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">【a】</mo>|【b】</mrow></mrow></semantics>

也就是说，我们想知道两个事物都为真的概率，假定被作为条件的一个是真的。

![A graphical depiction of conditional probability, showing how the conditional probability limits our analysis to a subset of the data.](../media/file40.png)

图6.3:条件概率的图形描述，显示了条件概率如何将我们的分析限制在数据的一个子集。

图形化地思考这一点会很有用。图 [6.3](#fig:conditionalProbability) 显示了一个流程图，描述了全部选民如何细分为共和党和民主党，以及条件概率(以政党为条件)如何根据他们的投票进一步细分每个政党的成员。





## 6.5 根据数据计算条件概率

我们也可以直接从数据中计算条件概率。假设我们对以下问题感兴趣:假设某人没有进行体育锻炼，那么他患糖尿病的概率是多少？——即<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>I</mi><mi>b</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="false" form="prefix">|</mo><mi>I</mi><mi>n</mi><mi>a</mi><mi>cNHANES数据集包括两个变量，解决这个问题的两个部分。第一个(`Diabetes`)询问该人是否被告知他们患有糖尿病，第二个(`PhysActive`)记录该人是否参加至少中等强度的运动、健身或娱乐活动。让我们首先计算简单概率，如表 [6.2](#tab:simpleProb) 所示。该表显示，NHANES数据集中有人患糖尿病的概率为. 1，有人不活动的概率为. 45。</mi></mrow></mrow></semantics></math>

<caption>Table 6.2: Summary data for diabetes and physical activity</caption>
| 回答 | 糖尿病 | p _糖尿病 | 生理活性物质 | P_PhysActive |
| --- | --- | --- | --- | --- |
| 不 | Four thousand eight hundred and ninety-three | Zero point nine | Two thousand four hundred and seventy-two | Zero point four five |
| 是 | Five hundred and fifty | Zero point one | Two thousand nine hundred and seventy-one | Zero point five five |

<caption>Table 6.3: Joint probabilities for Diabetes and PhysActive variables.</caption>
| 糖尿病 | 物理活性的 | n | 问题 |
| --- | --- | --- | --- |
| 不 | 不 | Two thousand one hundred and twenty-three | Zero point three nine |
| 不 | 是 | Two thousand seven hundred and seventy | Zero point five one |
| 是 | 不 | Three hundred and forty-nine | Zero point zero six |
| 是 | 是 | Two hundred and one | Zero point zero four |

来计算<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>I</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="false" form="prefix">|</mo><mi>I</mi><mi>n</mi><mi>a</mi><mi>c</mi>这些如表 <a xmlns:epub="http://www.idpf.org/2007/ops" href="#tab:jointProb">6.3</a> 所示。基于这些联合概率，我们可以计算出<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>I</mi><mi>a</mi><mi>b</mi><mi>e</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="false" form="prefix">|<mo stretchy="false" form="prefix">在计算机程序中做到这一点的一种方法是，首先确定每个个体的体力活动变量是否等于“否”,然后取这些真值的平均值。由于真/假值分别被大多数编程语言(包括R和Python)视为1/0，这允许我们通过简单地取代表其真值的逻辑变量的平均值来容易地识别简单事件的概率。然后，我们使用该值来计算条件概率，我们发现，假设某人不运动，那么他患糖尿病的概率是0.141。</mo></mo></mrow></mrow></semantics></math></mrow></mrow></semantics></math>





## 6.6 独立性

“独立”一词在统计学中有非常具体的含义，与该词的通常用法有些不同。两个变量之间的统计独立性意味着知道一个变量的值并不能告诉我们另一个变量的值。这可以表示为:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">P(A | B)= P(A)</annotation></mrow></semantics></math>

也就是说，给定B的某个值的概率恰好与A的总体概率相同。从这个角度看，我们看到现实世界中许多我们称之为“独立”的情况实际上在统计上并不独立。例如，目前有一小群加州公民正在采取行动，宣布成立一个名为杰斐逊的新的独立州，该州将包括北加州和俄勒冈州的一些县。如果发生这种情况，那么当前加州居民现在居住在杰斐逊州的概率将是<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">杰斐逊</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.014</mn></mrow><annotation encoding="application/x-tex">P(\ text {杰斐逊})=0.014</annotation></semantics></math> ，而他们将继续居住在杰斐逊州的概率新的州可能在政治上是独立的，但是他们在统计上是独立的，因为如果我们知道一个人是杰斐逊派，那么我们可以确定他们不是加利福尼亚人！也就是说，在普通语言中，独立性通常指的是具有排他性的集合，而统计独立性指的是无法从一个变量的值预测另一个变量的情况。例如，知道一个人的头发颜色不太可能告诉你他们更喜欢巧克力还是草莓冰淇淋。

让我们用NHANES的数据来看另一个例子:身体健康和精神健康是相互独立的吗？NHANES包括两个相关的问题:*physical active*，询问个人是否身体活跃，以及*daysmenthmethbad*，询问个人在过去30天中有多少天经历了糟糕的心理健康。让我们把任何一个在过去的一个月里有超过7天精神健康状况不佳的人都视为精神健康状况不佳。基于此，我们可以定义一个叫做 *badMentalHealth* 的新变量，作为一个逻辑变量，告诉我们每个人是否有超过7天的不良心理健康。我们可以首先汇总数据，显示有多少人属于这两个变量的每个组合(如表 [6.4](#tab:mhCounts) 所示)，然后除以观察总数，创建一个比例表(如表 [6.5](#tab:mhProps) 所示):

<caption>Table 6.4: Summary of absolute frequency data for mental health and physical activity.</caption>
| 物理活性的 | 心理健康状况不佳 | 精神健康 | 总数 |
| --- | --- | --- | --- |
| 不 | Four hundred and fourteen | One thousand six hundred and sixty-four | Two thousand and seventy-eight |
| 是 | Two hundred and ninety-two | One thousand nine hundred and twenty-six | Two thousand two hundred and eighteen |
| 总数 | Seven hundred and six | Three thousand five hundred and ninety | Four thousand two hundred and ninety-six |

<caption>Table 6.5: Summary of relative frequency data for mental health and physical activity.</caption>
| 物理活性的 | 心理健康状况不佳 | 精神健康 | 总数 |
| --- | --- | --- | --- |
| 不 | Zero point one | Zero point three nine | Zero point four eight |
| 是 | Zero point zero seven | Zero point four five | Zero point five two |
| 总数 | Zero point one six | Zero point eight four | One |

这向我们显示了落入每个单元的所有观察值的比例。但是，这里我们想知道的是心理健康不好的条件概率，取决于一个人是否身体活跃。为了计算这一点，我们将每个身体活动组除以其观察总数，这样每一行总计为一(如表 [6.6](#tab:condProb) 所示)。在这里，我们看到每个身体活动组的心理健康好坏的条件概率(在上面两行)以及心理健康好坏的总体概率在第三行。为了确定心理健康和身体活动是否是独立的，我们将比较不良心理健康的简单概率(在第三行)和假设一个人是身体活动的不良心理健康的条件概率(在第二行)。

<caption>Table 6.6: Summary of conditional probabilities for mental health given physical activity.</caption>
| 物理活性的 | 心理健康状况不佳 | 精神健康 | 总数 |
| --- | --- | --- | --- |
| 不 | Zero point two | Zero point eight | one |
| 是 | Zero point one three | Zero point eight seven | one |
| 总数 | Zero point one six | Zero point eight four | one |

心理健康不良的总体概率<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">心理健康不良</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\ text {心理健康不良})</annotation></semantics></math> 为0.16而条件概率<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">心理健康不良|身体活跃</mtext>因此，条件概率似乎比总体概率小一些，这表明它们不是独立的，尽管我们不能仅通过查看数字来确定，因为这些数字可能因我们样本中的随机可变性而不同。在本书的后面，我们将讨论统计工具，让我们直接测试两个变量是否独立。</mrow></mrow></semantics></math>





## 反转一个条件概率:贝叶斯法则

很多时候，我们知道<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A | B)</annotation></semantics></math>但是我们真正想知道的是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi>这种情况一般发生在医学筛查中，我们哪里知道<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">阳性检测结果|疾病</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">(P(\ text {阳性检测结果|疾病})</annotation></semantics></math> 但是我们想知道的是 <math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi> <mrow>例如，一些医生建议50岁以上的男性使用一种名为前列腺特异性抗原(PSA)的测试进行筛查，以筛查可能的前列腺癌。在测试被批准用于医疗实践之前，制造商需要测试测试性能的两个方面。首先，他们需要表明</mrow></mrow></semantics></math></mrow></mrow></semantics></math>*对*有多敏感——也就是说，当疾病出现时，发现它的可能性有多大: <math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mtext mathvariant="normal">敏感性</mtext><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">阳性检测|疾病</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\ text { sensitivity } = P(\他们还需要显示</annotation></semantics></math>*特异性*如何:即在没有疾病存在的情况下，给出阴性结果的可能性有多大: <math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mtext mathvariant="normal">特异性</mtext><mo>=</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">阴性测试|无疾病</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">对于PSA测试，我们知道灵敏度约为80%，特异性约为70%。然而，这些并没有回答医生想要为任何特定患者回答的问题:假设检测结果为阳性，他们实际上患癌症的可能性有多大？这就要求我们把定义灵敏度的条件概率反过来:而不是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>P</mi><mi>o</mi><mi>s</mi><mi>I</mi><mi>t</mi><mi>I</mi><mi>v</mi><mi>e <mi><mi>d</mi><mi>I</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>e</mi><mo stretchy="true" form="postfix">)</mo></mi></mi></mrow></mrow><annotation encoding="application/x-tex">P(正) 测试|疾病)</annotation></semantics></math> 我们想知道<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>d</mi><mi>I</mi><mi>s</mi><mi>e</mi><mi>a</mi><mi>s</mi><mi>e</mi></mrow></mrow></semantics></math></annotation></semantics></math>

为了反转一个条件概率，我们可以使用*贝叶斯法则*:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【b】</mi><mi>【a1】</mi></mrow></mrow></semantics>

根据我们在本章前面学到的概率规则，贝叶斯规则很容易推导出来(有关推导过程，请参见附录)。

如果我们只有两种结果，我们可以用更清晰的方式表达贝叶斯法则，用求和法则重新定义<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A)</annotation></semantics></math>:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【a】</mi><mo>=</mo></mrow> <mo>*</mo><mi>【p62】<mrow><mo stretchy="true" form="prefix">(</mo><mo><mi>【b】</mi><mo stretchy="true" form="postfix">)</mo></mo></mrow></mi></mrow></semantics>

利用这一点，我们可以重新定义贝叶斯法则:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【b】</mi><mi>【a1】</mi><mi><mo stretchy="true" form="postfix">)</mo></mi></mrow><mo><mi><mrow><mo stretchy="true" form="prefix">【b70】</mo></mrow></mi></mo></mrow></semantics>

我们可以将相关数字代入这个等式，以确定PSA结果阳性的个人实际患有癌症的可能性——但请注意，为了做到这一点，我们还需要知道该人患癌症的总体概率，我们通常称之为*基础率*。让我们假设一个60岁的老人，他在未来10年内患前列腺癌的概率是<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>c</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>r</mi>【T22)</mrow><mo>=</mo><mn>0.058</mn>使用我们上面概述的灵敏度和特异性值，我们可以计算出给定阳性测试的个体患癌症的可能性:</mrow></semantics></math>

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌|验</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">验|癌</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>*</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌 <mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">验|</mtext> <mtext mathvariant="normal">癌</mtext> <mo stretchy="true" form="postfix">)</mo></mrow></mtext></mrow> <mrow><mn>0.8</mn><mn>0.058</mn><mo>+</mo><mn>0.3</mn><mo>*</mo><mn>0.942</mn></mrow></mrow></mfrac><mo>=</mo> 许多人都这样做，事实上，有大量心理学文献表明，人们在判断时会系统性地忽略</mrow></semantics></math>*基础率*(即总体患病率)。





## 6.8 数据学习

另一种思考贝叶斯法则的方式是在数据的基础上更新我们的信念——也就是说，使用数据来了解世界。让我们再来看看贝叶斯法则:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【b】</mi><mi>【a1】</mi></mrow></mrow></semantics>

贝叶斯规则的不同部分有特定的名称，这与它们在使用贝叶斯规则更新我们的信念中的作用有关。我们从对B(<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(B)</annotation></semantics></math>)的概率的初步猜测开始，我们称之为*先验*概率。在PSA的例子中，我们使用基础率作为我们的先验，因为在我们知道测试结果之前，这是我们对个体患癌几率的最佳猜测。然后我们收集一些数据，在我们的例子中是测试结果。数据A与结果B的一致程度由<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A | B)</annotation></semantics></math>给出，我们称之为*可能性*。你可以认为这是数据的可能性，假设被测试的特定假设是真的。在我们的例子中，被测试的假设是个体是否患有癌症，可能性基于我们对测试灵敏度的了解(即，假设癌症存在，测试结果为阳性的概率)。分母(<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(A)</annotation></semantics></math>)被称为*边际似然*，因为它表达了数据的总体似然，是B的所有可能值的平均值(在我们的例子中是疾病存在和疾病不存在)。胜负向左(<math display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="prefix">|</mo>|<mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(B | A)</annotation></semantics></math>)被称为*后路*——因为它

有另一种写贝叶斯规则的方式可以使这一点更清楚:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【b】</mi><mi>【a1】</mi></mrow></mrow></semantics>

左边的部分(<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><annotation encoding="application/x-tex">\ frac { P(A | B 而右边的部分(<math xmlns:epub="http://www.idpf.org/2007/ops" display="inline"><semantics><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>B</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(B)</annotation></semantics></math>)告诉我们，在我们对数据一无所知之前，我们认为B有多大的可能性。 这更清楚地表明，贝叶斯定理的作用是基于数据比总体更有可能给出B的程度来更新我们的先验知识。如果假设在给定数据的情况下比一般情况下更有可能，那么我们会增加对假设的信任；如果给定的数据不太可能，那么我们减少我们的信念。</annotation></semantics></math>





## 6.9 优势比和优势比

上一部分的结果显示，根据阳性PSA测试结果，个人患癌症的可能性仍然相当低，尽管这是我们知道测试结果之前的两倍多。我们常常希望更直接地量化概率之间的关系，这可以通过将它们转换成表示某事发生或不发生的相对可能性的*赔率*来实现:
<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mtext mathvariant="normal">A的赔率</mtext><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>A</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi></mrow></mfrac></mrow></semantics></math>

在我们的PSA示例中，患癌症的几率(假设检测结果为阳性)为:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mtext mathvariant="normal">癌症几率</mtext><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌症</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌症</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo></mrow></semantics></math>

这告诉我们，即使测试呈阳性，患癌症的几率还是相当低的。作为比较，在一次掷骰子中掷出6的几率是:

<math display="block"><semantics><mrow><mtext mathvariant="normal">赔率6</mtext><mo>=</mo><mfrac><mn>1</mn><mn>5</mn></mfrac><mo>=</mo><mn>0.2</mn></mrow><annotation encoding="application/x-tex">\ text {赔率6 } = \ frac { 1 } { 5 } = 0.2</annotation></semantics></math>

此外，这也是为什么许多医学研究人员越来越警惕对相对罕见的疾病使用广泛的筛查测试的原因；大多数阳性结果将被证明是假阳性，导致不必要的后续检查和可能的并发症，更不用说增加病人的压力。

我们还可以通过计算所谓的*比值比*来比较不同的概率——听起来确实如此。例如，假设我们想知道阳性测试会增加个人患癌症的几率有多大。我们可以首先计算出*先验概率*——也就是说，在我们知道这个人检测呈阳性之前的概率。这些是使用基本费率计算的:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mo>=</mo><mfrac><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>P</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">癌</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo></mrow></semantics></math>

然后，我们可以将这些与后验概率进行比较，后验概率是使用后验概率计算的:

<math xmlns:epub="http://www.idpf.org/2007/ops" display="block"><semantics><mrow><mtext mathvariant="normal">赔率</mtext> <mo>=</mo> <mfrac><mtext mathvariant="normal">后验赔率</mtext> <mtext mathvariant="normal">先验赔率</mtext></mfrac><mo>=</mo><mfrac><mn>0.16</mn><mn>0.061</mn></mfrac><mo>=</mo><mn>2.62</mn></mrow><annotation encoding="application/x-tex">\ text { odds ratio } = \ frac { \ text {后验赔率} } { \ text {后验赔率}</annotation></semantics></math>

这告诉我们，在测试结果为阳性的情况下，患癌症的几率增加了2.62倍。优势比是我们稍后称之为*效应大小*的一个例子，这是一种量化任何特定统计效应有多大的方法。





## 6.10 概率是什么意思？

你可能会觉得，根据检测结果谈论一个人患癌症的可能性有点奇怪；毕竟，这个人要么得了癌症，要么没有。历史上，概率有两种不同的解释方式。第一种(被称为*频率主义者*解释)从长期频率的角度解释概率。例如，在抛硬币的情况下，在大量抛硬币后，它将反映头部的相对频率。虽然这种解释对于像掷硬币一样可以重复多次的事件可能有意义，但对于只会发生一次的事件，如个人生活或特定的总统选举，就不那么有意义了；正如经济学家约翰·梅纳德·凯恩斯(John Maynard Keynes)的名言，“从长期来看，我们都会死。”

概率的另一种解释(称为*贝叶斯*解释)是对特定命题的信任程度。如果我问你“美国在2040年重返月球的可能性有多大”，你可以基于你的知识和信念给出这个问题的答案，即使没有相关的频率来计算频率主义者的概率。我们通常框定主观概率的一种方式是根据一个人接受特定赌博的意愿。例如，如果你认为美国在2040年登陆月球的概率是0.1(即赔率为9比1)，那么这意味着你应该愿意接受一场赌博，如果事件发生，赔率将超过9比1。

正如我们将会看到的，这两种不同的概率定义与统计学家考虑检验统计假设的两种不同方式非常相关，我们将在后面的章节中遇到。





## 6.11 学习目标

阅读完本章后，您应该能够:

*   描述所选随机实验的样本空间。
*   计算一组给定事件的相对频率和经验概率
*   计算单一事件、互补事件以及事件集合的联合和交集的概率。
*   描述一下大数定律。
*   描述概率和条件概率的区别
*   描述统计独立性的概念
*   使用贝叶斯定理计算逆条件概率。





## 6.12 建议读数

*   《酒鬼的行走:随机性如何主宰我们的生活》，作者列纳德·蒙洛迪诺
*   佩尔西·戴康尼斯和布莱恩·斯凯尔姆的《关于机遇的十个伟大想法》





## 6.13 附录



### 6.13.1 贝叶斯法则的推导

首先，记住计算条件概率的规则:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">【a】</mo>|【b】</mrow></mrow></semantics>

我们可以重新排列，得到使用条件的计算联合概率的公式:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">【a】</mo><mi>【b】</mi></mrow></mrow></semantics>

利用这一点，我们可以计算逆概率:

<semantics><mrow><mi>【p】</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>【b】</mi><mi>【a1】</mi></mrow></mrow><mo>=</mo><mfrac><mrow><mi>【p54】<mrow><mo stretchy="true" form="prefix">a</mo></mrow></mi></mrow></mfrac></semantics>



</mi></mi></mi></mrow></semantics></math> 



